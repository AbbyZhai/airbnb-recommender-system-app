{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "73c8ad2f1e63492795b5947592cb4c42",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## A combined recommender system based on numeric and text listing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "480ca3289e5c4084a2859559115106a0",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import sparse\n",
    "from scipy.stats import kurtosis, skew\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=DeprecationWarning)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "RANDOM_STATE= 42\n",
    "\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from kneed import KneeLocator\n",
    "from pickle import dump, load\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "bcab01e19f2d4d4ba2a3ea5d753bbc04",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "##### filtering listing data \n",
    "\n",
    "def get_data(price_range,num_of_beds,num_of_bedrooms,num_of_bathrooms):\n",
    "    \n",
    "    df = pd.read_pickle('../data/data_cleaned/cleaned_listing_finalized_for_streamlit.zip')\n",
    "\n",
    "        \n",
    "    if len(df.loc[(df['price']>price_range[0])&(df['price']<=price_range[1])])!=0:\n",
    "        df_filter = df.loc[(df['price']>=price_range[0])&(df['price']<=price_range[1])]\n",
    "        \n",
    " \n",
    "    else:\n",
    "        df_filter = df\n",
    "        \n",
    "    if len(df_filter.loc[df_filter['beds']==num_of_beds])!=0:\n",
    "        df_filter = df_filter.loc[df_filter['beds']==num_of_beds]\n",
    "        \n",
    "   \n",
    "    else:\n",
    "        df_filter = df_filter\n",
    "        \n",
    "\n",
    "\n",
    "    if len(df_filter.loc[df_filter['bedrooms']==num_of_bedrooms])!=0:\n",
    "        df_filter = df_filter.loc[df_filter['bedrooms']==num_of_bedrooms]\n",
    "        \n",
    "      \n",
    "    else:\n",
    "        df_filter = df_filter\n",
    "        \n",
    "\n",
    "\n",
    "    if len(df_filter.loc[df_filter['bathrooms_count']==num_of_bathrooms])!=0:\n",
    "        df_filter = df_filter.loc[df_filter['bathrooms_count']==num_of_bathrooms]\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        df_filter = df_filter\n",
    "        \n",
    "                    \n",
    "    return df_filter\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "3b3353745b7e4826b842a29866104635",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "price_range = (50,500)\n",
    "num_of_beds = 1\n",
    "num_of_bedrooms = 1\n",
    "num_of_bathrooms = 1\n",
    "input_query = \"I like a room with a swimming pool\"\n",
    "\n",
    "\n",
    "## dataframe with satisfying the filter queries\n",
    "filter_df = get_data(price_range,num_of_beds,num_of_bedrooms,num_of_bathrooms)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "45efaf8e66f24efe8865bb57a7f98814",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "##### Cosine similarity\n",
    "\n",
    "major_cluster = filter_df['cluster'].value_counts().sort_values(ascending=False).index[0]\n",
    "cosine_similarity_col = ['host_response_rate', 'host_acceptance_rate',\n",
    "       'host_is_superhost', 'host_has_profile_pic', 'host_identity_verified',\n",
    "       'accommodates', 'bedrooms', 'beds', 'price', 'minimum_nights',\n",
    "       'maximum_nights', 'has_availability', 'number_of_reviews',\n",
    "       'review_scores_rating', 'review_scores_accuracy',\n",
    "       'review_scores_cleanliness', 'review_scores_checkin',\n",
    "       'review_scores_communication', 'review_scores_location',\n",
    "       'review_scores_value', 'has_license', 'instant_bookable',\n",
    "       'calculated_host_listings_count',\n",
    "       'calculated_host_listings_count_entire_homes',\n",
    "       'calculated_host_listings_count_private_rooms',\n",
    "       'calculated_host_listings_count_shared_rooms', 'reviews_per_month',\n",
    "       'bathrooms_count', 'amenities_count', 'host_operate_years', 'polarity']\n",
    "\n",
    "similarity_df = filter_df.loc[filter_df['cluster']==major_cluster][cosine_similarity_col]\n",
    "similarity_df = similarity_df.fillna(0)\n",
    "num_similarity = cosine_similarity(similarity_df)\n",
    "\n",
    "\n",
    "########################################################################################################\n",
    "##### build up num-based model\n",
    "\n",
    "model_columns_all = list(filter_df.columns.values)\n",
    "\n",
    "ui_display_columns = ['cluster',\n",
    "                      'listing_id',               \n",
    "                      'listing_url',                                      \n",
    "                      'listing_name',                                      \n",
    "                      'price',\n",
    "                      'beds',\n",
    "                      'bedrooms',\n",
    "                      'bathrooms_count',\n",
    "                      'description',                                      \n",
    "                      'room_type',                                      \n",
    "                      'property_type',                                      \n",
    "                      'neighborhood_overview',                                      \n",
    "                      'neighbourhood_cleansed',                                      \n",
    "                      'neighbourhood_group_cleansed',\n",
    "                      'amenities',                                      \n",
    "                      'number_of_reviews','review_scores_rating','host_about']\n",
    "\n",
    "iloc_cols = [model_columns_all.index(x) for x in ui_display_columns]\n",
    "\n",
    "\n",
    "def get_num_recommendations(df, similarity, n, listing_id=None, listing_url=None, query_element=None):\n",
    "\n",
    "    # convert query into and a similarity matrix row index\n",
    "    item_index = None\n",
    "    try:    \n",
    "        if listing_id is not None:\n",
    "            item_index = df['listing_id'].tolist().index(listing_id)\n",
    "        elif listing_url is not None:\n",
    "            item_index = df['listing_url'].tolist().index(listing_url)\n",
    "        elif query_element is not None:\n",
    "            item_index = query_element\n",
    "    except ValueError as error:\n",
    "        print(error)\n",
    "    \n",
    "    if len(df)>=n:\n",
    "        # get the top n similar items\n",
    "        top_idx = np.argsort(similarity[item_index])[::-1][:n]\n",
    "        result_df = df.iloc[top_idx, iloc_cols]\n",
    "        # add in similarity score as a column\n",
    "        top_scores = [similarity[item_index][x] for x in top_idx]\n",
    "        result_df.insert(loc=2, column='similarity', value=top_scores)\n",
    "        \n",
    "    else:\n",
    "        # get the top n similar items\n",
    "        top_idx = np.argsort(similarity[item_index])[::-1]\n",
    "        result_df = df.iloc[top_idx, iloc_cols]\n",
    "        # add in similarity score as a column\n",
    "        top_scores = [similarity[item_index][x] for x in top_idx]\n",
    "        result_df.insert(loc=0, column='similarity', value=top_scores)\n",
    "        \n",
    "    result_df = result_df.reset_index().iloc[:,1:]\n",
    "    result_df.index = np.arange(1,len(result_df)+1)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "7ca95a44628644a5aba9ec0cac3cc251",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "##### build up text-based model\n",
    "\n",
    "\n",
    "##### prepare stopword set\n",
    "added_stopwords = [\"can't\",'t', 'us', 'say','would', 'also','within','stay', 'since']\n",
    "nltk_STOPWORDS = set(stopwords.words(\"english\"))\n",
    "nltk_STOPWORDS.update(added_stopwords)\n",
    "\n",
    "\n",
    "##### preprocess input query\n",
    "\n",
    "def preprocess_text(text, stopwords = nltk_STOPWORDS, stem=False, lemma=False):\n",
    "    # clean the text\n",
    "    text = text.lower()\n",
    "    # remove html and all other sybols\n",
    "    text = re.sub(\"(<.*?>)|([^0-9A-Za-z \\t])\",\"\",text)\n",
    "    text = re.sub(\"(br)\", '', text)\n",
    "    # tokenize the text\n",
    "    text = word_tokenize(text)\n",
    "    # remove stopwords and non alpha words\n",
    "    text = [word for word in text if word not in stopwords]\n",
    "    # get the root of word\n",
    "    if stem == True:\n",
    "        stemmer = PorterStemmer()\n",
    "        text = [stemmer.stem(word) for word in text]\n",
    "    # normalize the word\n",
    "    if lemma == True:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    # list to string\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "##### Vectorize data\n",
    "\n",
    "def vectorize_data(corpus):\n",
    "    # TfidfVectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "                                    ngram_range = (1,2),\n",
    "                                    stop_words='english')\n",
    "    # update: use todense() and np.asarray to avoid error in streamlit app\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "    return tfidf_vectorizer, tfidf_matrix\n",
    "\n",
    "corpus = filter_df['content'].values\n",
    "tfidf_vectorizer, tmatrix = vectorize_data(corpus)\n",
    "\n",
    "##### get similarity\n",
    "\n",
    "def extract_best_indices(similarity, top_n, mask=None):\n",
    "    \"\"\"\n",
    "    Use sum of the cosine distance over all tokens and return best mathes.\n",
    "    m (np.array): cos matrix of shape (nb_in_tokens, nb_dict_tokens)\n",
    "    topk (int): number of indices to return (from high to lowest in order)\n",
    "    \"\"\"\n",
    "    # return the sum on all tokens of consine for the input query\n",
    "    if len(similarity.shape) > 1:\n",
    "        cos_sim = np.mean(similarity, axis=0)\n",
    "    else:\n",
    "        cos_sim = similarity\n",
    "    index = np.argsort(cos_sim)[::-1]\n",
    "    if mask is not None:\n",
    "        assert mask.shape == m.shape\n",
    "        mask = mask[index]\n",
    "    else:\n",
    "        mask = np.ones(len(cos_sim))\n",
    "    mask = np.logical_or(cos_sim[index] != 0, mask) #eliminate 0 cosine distance\n",
    "    best_index = index[mask][:top_n]\n",
    "    return best_index\n",
    "\n",
    "\n",
    "##### get recommendations\n",
    "\n",
    "def get_text_recommendations(df, input_query, _tfidf_matrix, n=5):\n",
    "\n",
    "    # embed input query\n",
    "    tokens = preprocess_text(input_query,stopwords = nltk_STOPWORDS, stem=False, lemma=True).split()\n",
    "    query_vector = tfidf_vectorizer.transform(tokens)\n",
    "\n",
    "    # get similarity\n",
    "    similarity = cosine_similarity(query_vector, _tfidf_matrix)\n",
    "\n",
    "    # best cosine distance for each token independantly\n",
    "    best_index = extract_best_indices(similarity, top_n=n)\n",
    "\n",
    "    # return the top n similar listing ids and raw comments\n",
    "    result_df = df.loc[best_index,:]\n",
    "    result_df = result_df.loc[:, ['cluster',\n",
    "                                  'listing_id',\n",
    "                                  'listing_url',\n",
    "                                  'listing_name',\n",
    "                                  'price',\n",
    "                                  'beds',\n",
    "                                  'bedrooms',\n",
    "                                  'bathrooms_count',\n",
    "                                  'description',\n",
    "                                  'room_type',\n",
    "                                  'property_type',\n",
    "                                  'neighborhood_overview',\n",
    "                                  'neighbourhood_cleansed',\n",
    "                                  'neighbourhood_group_cleansed',\n",
    "                                  'amenities',\n",
    "                                  'number_of_reviews','review_scores_rating','host_about']]\n",
    "    result_df = result_df.reset_index().iloc[:,1:]\n",
    "    result_df.index = np.arange(1,len(result_df)+1)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "f69e4a8fc34943b9a8748b205c68b2f1",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "##### build up the combined model\n",
    "\n",
    "def get_recommendation(df,input_query,_tfidf_matrix, n):\n",
    "    if input_query == \"\":\n",
    "        rec_df = df.loc[df['cluster']==major_cluster]\n",
    "        select_listing_id = rec_df['listing_id'].iloc[0]\n",
    "        index = rec_df['listing_id'].tolist().index(select_listing_id)\n",
    "        recomended_listings = get_num_recommendations(rec_df, num_similarity, n, listing_id=select_listing_id)\n",
    "        \n",
    "    else:\n",
    "        # get corpus\n",
    "        df = df.reset_index()\n",
    "        recomended_listings = get_text_recommendations(df, input_query, _tfidf_matrix, n)\n",
    "        \n",
    "    return recomended_listings\n",
    "\n",
    "\n",
    "recomended_listings_update = get_recommendation(filter_df,input_query,tmatrix, 5)\n",
    "recomended_listings_update\n",
    "\n",
    "\n",
    "########################################################################################################\n",
    "# add review sentiment plot for the recommended listings #\n",
    "\n",
    "\n",
    "def get_review_data():\n",
    "    # directly read the saved cleaned_review_with_polarity_and_topic dataset\n",
    "    review_df = pd.read_pickle('../data/data_cleaned/cleaned_review_with_polarity_and_topic.zip')\n",
    "    return review_df\n",
    "review_df = get_review_data()\n",
    "\n",
    "# make plot\n",
    "# notice: altair can only take <=5000 rows, so cannot show all listings at once\n",
    "\n",
    "def plot_listing_sentiment_over_time(df,listing_id = None):\n",
    "    sub_df = df[df['listing_id'].isin(listing_id)]\n",
    "    return alt.Chart(sub_df, width=500).mark_line().encode(\n",
    "                x='year(date):T',\n",
    "                y='mean(polarity)',\n",
    "                color=alt.Color('listing_id:O', scale=alt.Scale(scheme= 'dark2'))\n",
    "            ).interactive()\n",
    "\n",
    "# plot the sentiment changes over time by year for the recommended listings\n",
    "rec_listing_ids = recomended_listings_update['listing_id'].values\n",
    "sentiment_plot = plot_listing_sentiment_over_time(review_df, rec_listing_ids)\n",
    "sentiment_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "2d573cc9c1604c2296a379314e1377bf",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "7c143c3beee64d96a302fd24a2ab1444",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=f2a50dc6-ff6a-45ff-9dbe-d7a35bd1e393' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "53dd9391232d4c61a415f8a6902c29e1",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
